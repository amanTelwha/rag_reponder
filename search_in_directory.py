import to_lower
from fileutils import read_file
#https://chat.openai.com/share/32f97de9-8681-4ef7-8ce3-d670600faf15

import os

def search_in_directory(search_term, directory_path, case_sensitive=False):
    """
    Search for a given term in all text files within a specified directory.
    
    Args:
    search_term (str): The term to search for.
    directory_path (str): The path to the directory containing the files to search.
    case_sensitive (bool): If True, the search will be case-sensitive; otherwise, it will be case-insensitive.
    
    Returns:
    list: A list of filenames containing the search term.
    """


    matching_files = []
    
    # Adjust the search term for case sensitivity
    if not case_sensitive:
        search_term = to_lower.to_lowercase(search_term)
    
    # Walk through the directory
    for dirpath, dirnames, filenames in os.walk(directory_path):
        for filename in filenames:
            # Construct full file path
            file_path = os.path.join(dirpath, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                    # Adjust file content for case sensitivity
                    if not case_sensitive:
                        content = content.lower()
                    
                    # Check if search term is in content
                    if search_term in content:
                        matching_files.append(filename)
            except Exception as e:
                print(f"Failed to read {file_path}: {e}")
    
    return matching_files


import logging
from openai import  OpenAIError
# Example usage:
# result = search_in_directory("example", "/path/to/directory", case_sensitive=False)
# print(result)

def rag_responder(query_template,kb,config):
    """
    Responds to a query using the Retrieval-Augmented Generation (RAG) model.
    
    Args:
    query_template (str): The template for the query.
    kb (str): The knowledge base to search for the answer.
    llm_config (dict): The configuration for the LLM model.
    
    Returns:
    str: The response generated by the RAG model.
    """
    query=get_prompt(query_template,config)
    documents=search_in_directory(query,kb,case_sensitive=False)
    message=None
    for document in documents:
        with open(os.path.join(kb,document),'r',encoding='utf-8') as file:
            content= file.read()
            try:
                response=query_llm(query,content)
                if response.status=="Success":
                    message=response.answer
                    status="Success"
                    break
            except OpenAIError as e:
                logging.error(f"Error communicating with llm service: ")
                raise AnswerServiceError("Error communicating with llm service")
    if message is None:
        message="Sorry, I couldn't find an answer to your question."
        status="Failed"
    return Response(status,message)


       
            
class AnswerServiceError(Exception):
    pass    
            

class Response:
    def __init__(self,status,answer):
        self.status=status
        self.answer=answer    

def query_llm(promt,context):
    raise NotImplementedError("This function is not yet implemented.")

def get_prompt(query_template, config):
    """
    Form a query string from a template by replacing placeholders with provided parameters.
    
    Args:
    query_template (str): The template for the query.
    query_params (dict): A dictionary containing parameters to replace in the query template.
    
    Returns:
    str: The query string with placeholders replaced by parameters.
    """
    raise NotImplementedError("This function is not yet implemented.")